{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning # To make code shorter"
      ],
      "metadata": {
        "id": "vESfQJ3t74RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooA7MgLM5dl_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import MNIST\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
        "NUM_WORKERS = int(os.cpu_count()/2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "  def __init__(self, data_dir = \"./data\", batch_size = BATCH_SIZE, num_workers = NUM_WORKERS):\n",
        "    super().__init__()\n",
        "    self.data_dir = data_dir\n",
        "    self.batch_size = batch_size\n",
        "    self.num_workers = num_workers\n",
        "    self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.1307,), (0.3081,)),\n",
        "        ])\n",
        "  def prepare_data(self):\n",
        "    MNIST(self.data_dir, train = True, download = True)\n",
        "    MNIST(self.data_dir, train = False, download = True)\n",
        "\n",
        "  def setup(self, stage = None):\n",
        "    # Assign train/val datasets\n",
        "    if stage== \"fit\" or stage is None:\n",
        "      mnist_full = MNIST(self.data_dir, train= True, transform = self.transform)\n",
        "      self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
        "    if stage ==\"test\" or stage is None:\n",
        "      self.mnist_test = MNIST(self.data_dir, train= False, transform = self.transform)\n",
        "  def train_dataloader(self):\n",
        "    return DataLoader(self.mnist_train, batch_size = self.batch_size, num_workers = self.num_workers)\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    return DataLoader(self.mnist_val, batch_size = self.batch_size, num_workers = self.num_workers)\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    return DataLoader(self.mnist_test, batch_size = self.batch_size, num_workers = self.num_workers)\n"
      ],
      "metadata": {
        "id": "DqMUuhpk8fcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detective: fake or real --> 1 output[0,1]\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
        "    self.conv2 = nn.Conv2d(10, 20, kernel_size = 5)\n",
        "    self.conv2_drop = nn.Dropout2d()\n",
        "    self.fc1 = nn.Linear(320, 50)\n",
        "    self.fc2 = nn.Linear(50, 1) # 1 ; the only output\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
        "    x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
        "\n",
        "    # Flatten the tensor so it can be fed into the FC layers\n",
        "    x = x.view(-1,320)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.dropout(x, training = self.training)\n",
        "    x = self.fc2(x)\n",
        "    return torch.sigmoid(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Th60xHsxEdQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Fake Data : Output like real data [1,28,28] and values -1,1\n",
        "# Based on latent_dim value, we upsample an outpit that is in the shape of original images [1,28,28] and values -1,1\n",
        "\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, latent_dim):\n",
        "    super().__init__()\n",
        "    self.lin1 = nn.Linear(latent_dim, 7*7*64) # [n,256, 7,7]\n",
        "    self.ct1 = nn.ConvTranspose2d(64, 32, 4, stride = 2) # [n, 64, 16, 16]\n",
        "    self.ct2 = nn.ConvTranspose2d(32, 16, 4, stride = 2) # [n, 16, 34, 34]\n",
        "    self.conv = nn.Conv2d(16, 1, kernel_size = 7) # [n, 1, 28, 28]\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Pass latent space input into linear layer and reshape\n",
        "    x = self.lin1(x)\n",
        "    x = F.relu(x)\n",
        "    x = x.view(-1, 64, 7, 7) # 256\n",
        "\n",
        "    # Upsample (transposed conv) 16x16 (64 feature maps)\n",
        "    x = self.ct1(x)\n",
        "    x = F.relu(x)\n",
        "    # Upsample to 34x34 (16 feature maps)\n",
        "    x = self.ct2(x)\n",
        "    x = F.relu(x)\n",
        "\n",
        "    # Convolution to 28x28(1 feature map)\n",
        "    return self.conv(x)"
      ],
      "metadata": {
        "id": "EQ6vwdaKKt0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GAN(pl.LightningModule):\n",
        "    def __init__(self, latent_dim=100, lr=0.0002):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.generator = Generator(latent_dim=self.hparams.latent_dim)\n",
        "        self.discriminator = Discriminator()\n",
        "        self.validation_z = torch.randn(6, self.hparams.latent_dim)\n",
        "        self.automatic_optimization = False  # Enable manual optimization\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.generator(z)\n",
        "\n",
        "    def adversarial_loss(self, y_hat, y):\n",
        "        return F.binary_cross_entropy(y_hat, y)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        real_imgs, _ = batch\n",
        "        opt_g, opt_d = self.optimizers()  # Access optimizers\n",
        "\n",
        "        # Sample noise\n",
        "        z = torch.randn(real_imgs.shape[0], self.hparams.latent_dim).type_as(real_imgs)\n",
        "\n",
        "        # Train Generator: max log(D(G(z)))\n",
        "        fake_imgs = self(z)  # Generate fake images\n",
        "        y_hat = self.discriminator(fake_imgs)\n",
        "        y = torch.ones(real_imgs.size(0), 1).type_as(real_imgs)\n",
        "        g_loss = self.adversarial_loss(y_hat, y)\n",
        "\n",
        "        # Optimize Generator\n",
        "        opt_g.zero_grad()\n",
        "        self.manual_backward(g_loss)\n",
        "        opt_g.step()\n",
        "\n",
        "        # Train Discriminator: max log D(x) + log (1 - D(G(z)))\n",
        "        y_hat_real = self.discriminator(real_imgs)\n",
        "        y_real = torch.ones(real_imgs.size(0), 1).type_as(real_imgs)\n",
        "        real_loss = self.adversarial_loss(y_hat_real, y_real)\n",
        "\n",
        "        y_hat_fake = self.discriminator(fake_imgs.detach())\n",
        "        y_fake = torch.zeros(real_imgs.size(0), 1).type_as(real_imgs)\n",
        "        fake_loss = self.adversarial_loss(y_hat_fake, y_fake)\n",
        "\n",
        "        d_loss = (real_loss + fake_loss) / 2\n",
        "\n",
        "        # Optimize Discriminator\n",
        "        opt_d.zero_grad()\n",
        "        self.manual_backward(d_loss)\n",
        "        opt_d.step()\n",
        "\n",
        "        log_dict = {\"g_loss\": g_loss, \"d_loss\": d_loss}\n",
        "        self.log_dict(log_dict)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.hparams.lr\n",
        "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr)\n",
        "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr)\n",
        "        return [opt_g, opt_d]\n",
        "\n",
        "    def plot_imgs(self):\n",
        "        z = self.validation_z.type_as(self.generator.model[0].weight)\n",
        "        sample_imgs = self(z).cpu()\n",
        "\n",
        "        print(\"epoch\", self.current_epoch)\n",
        "        fig = plt.figure()  # Fixed typo: fiqure -> figure\n",
        "        for i in range(sample_imgs.size(0)):\n",
        "            plt.subplot(2, 3, i + 1)\n",
        "            plt.tight_layout()\n",
        "            plt.imshow(sample_imgs.detach()[i, 0, :, :], cmap=\"gray_r\", interpolation=\"none\")\n",
        "            plt.title(\"Generated Data\")\n",
        "            plt.xticks([])\n",
        "            plt.yticks([])\n",
        "            plt.axis(\"off\")\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "QGEvETJJvcu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data_Module = MNISTDataModule()\n",
        "model = GAN()"
      ],
      "metadata": {
        "id": "yaOy9ldIe_96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.plot_imgs"
      ],
      "metadata": {
        "id": "nPPRk6WvfL4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = pl.Trainer(max_epochs = 20, devices = AVAIL_GPUS)\n",
        "trainer.fit(model,Data_Module)"
      ],
      "metadata": {
        "id": "sLbdErwpfak_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}